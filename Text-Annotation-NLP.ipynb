{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8aab7280",
   "metadata": {},
   "source": [
    "# **introduction to TextBlob in Python**\n",
    "\n",
    "---\n",
    "\n",
    "ðŸ‘‹ **Hey there!** Ready to explore how Python can understand human language? Letâ€™s meet your new buddy: **TextBlob**!\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ¤– What is TextBlob?\n",
    "\n",
    "Think of **TextBlob** as your smart assistant that can read and analyze text *almost like a human*â€”but faster and in Python! It helps with:\n",
    "\n",
    "* Figuring out what someone *feels* in a sentence (positive or negative?) ðŸŽ­\n",
    "* Splitting text into words and sentences âœ‚ï¸\n",
    "* Correcting spelling mistakes âœï¸\n",
    "* Translating from one language to another ðŸŒ\n",
    "* And even fixing grammar!\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§  Why use TextBlob?\n",
    "\n",
    "Because itâ€™s **super beginner-friendly**! You donâ€™t need to write complex code. Just one or two lines, and boomâ€”youâ€™ve got insights from text.\n",
    "\n",
    "---\n",
    "\n",
    "### âœ¨ Letâ€™s try it out!\n",
    "\n",
    "```python\n",
    "from textblob import TextBlob\n",
    "\n",
    "text = \"I really love programming with Python!\"\n",
    "blob = TextBlob(text)\n",
    "\n",
    "print(\"Words:\", blob.words)\n",
    "print(\"Sentiment:\", blob.sentiment)\n",
    "```\n",
    "\n",
    "ðŸ“Œ Output:\n",
    "\n",
    "```\n",
    "Words: ['I', 'really', 'love', 'programming', 'with', 'Python']\n",
    "Sentiment: Sentiment(polarity=0.9, subjectivity=0.6)\n",
    "```\n",
    "\n",
    "ðŸŽ¯ That `polarity` tells us the sentence is **very positive**!\n",
    "\n",
    "---\n",
    "\n",
    "Ready to explore more like **spelling correction** or **translation**?\n",
    "\n",
    "Try these next:\n",
    "\n",
    "```python\n",
    "TextBlob(\"I havv goo grammer.\").correct()\n",
    "blob.translate(to='hi')  # Translates to Hindi\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "TextBlob turns text into insights, effortlessly. A perfect tool if you're starting out in **Natural Language Processing**!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b03aaba",
   "metadata": {},
   "source": [
    "# 1. Text Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fc38ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "text = \"I love programming in Python. It's such a versatile language!\"\n",
    "blob = TextBlob(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fbcff2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polarity: 0.25\n",
      "Subjectivity: 0.55\n"
     ]
    }
   ],
   "source": [
    "# Get Polarity and Subjectivity\n",
    "print(\"Polarity:\", blob.sentiment.polarity)\n",
    "print(\"Subjectivity:\", blob.sentiment.subjectivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cd8d04",
   "metadata": {},
   "source": [
    "# 2. Detect posttive or negative sentiment in a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "681c7b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n",
      "Positive\n"
     ]
    }
   ],
   "source": [
    "def detect_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    if blob.sentiment.polarity > 0:\n",
    "        return \"Positive\"\n",
    "    elif blob.sentiment.polarity < 0:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "    \n",
    "print(detect_sentiment(\"i hate the interface\"))\n",
    "print(detect_sentiment('I love design'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9b79b0",
   "metadata": {},
   "source": [
    "# 3. Sentiment of Multiple Sentences (List of Reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2595668b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'I Love the product!' - Sentiment: Positive\n",
      "'Wrost camera ever!' - Sentiment: Neutral\n",
      "'Battery life is ok' - Sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "reviews = [\"I Love the product!\",\"Wrost camera ever!\",\"Battery life is ok\"]\n",
    "for review in reviews:\n",
    "    sentiment = TextBlob(review).sentiment.polarity\n",
    "    print(f\"'{review}' - Sentiment: {'Positive' if sentiment > 0 else 'Negative' if sentiment < 0 else 'Neutral'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7148c49",
   "metadata": {},
   "source": [
    "# 4. Count Positive/Negative/Netural in Text list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa38bc74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts: {'Positive': 3, 'Negative': 1, 'Neutral': 0}\n"
     ]
    }
   ],
   "source": [
    "reviews=[\"Great product!\",\"Worst experience ever!\",\"It's okay, not bad but not great.\", \"Not Bad\"]\n",
    "\n",
    "counts = {\"Positive\": 0, \"Negative\": 0, \"Neutral\": 0}\n",
    "for review in reviews:\n",
    "    polarity = TextBlob(review).sentiment.polarity\n",
    "    if polarity > 0:\n",
    "        counts[\"Positive\"] += 1\n",
    "    elif polarity < 0:\n",
    "        counts[\"Negative\"] += 1\n",
    "    else:\n",
    "        counts[\"Neutral\"] += 1\n",
    "print(\"Counts:\", counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad649205",
   "metadata": {},
   "source": [
    "# 5. Simple GUI For Sentiment (Using Tkinter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae39947c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tkinter import *\n",
    "# from textblob import TextBlob\n",
    "\n",
    "# def analyze():\n",
    "#     text = entry.get()\n",
    "#     blob = TextBlob(text)\n",
    "#     result.set(f\"Polarity: {blob.sentiment.polarity:.2f}\")\n",
    "\n",
    "# root = Tk()\n",
    "# root.title(\"Sentiment Analyzer\")\n",
    "\n",
    "# entry = Entry(root, width=40)\n",
    "# entry.pack(pady=10)\n",
    "\n",
    "\n",
    "# Button(root, text=\"Analyze\", command=analyze).pack()\n",
    "\n",
    "# result = StringVar()\n",
    "# Label(root, textvariable=result).pack(pady=5)\n",
    "\n",
    "# root.mainloop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0613c12",
   "metadata": {},
   "source": [
    "# Sentiment Annotation Types â€“ Python Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93565dd",
   "metadata": {},
   "source": [
    "In **TextBlob**, **polarity** is a number that tells you how **positive or negative** a piece of text is.\n",
    "\n",
    "### ðŸ“Š Polarity Score Range:\n",
    "\n",
    "* **-1.0** â†’ *very negative*\n",
    "* **0.0** â†’ *neutral*\n",
    "* **+1.0** â†’ *very positive*\n",
    "\n",
    "### ðŸ§  How it works:\n",
    "\n",
    "TextBlob uses a built-in sentiment lexicon (a list of words with associated emotions) to calculate the average sentiment of words in a sentence.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§ª Example:\n",
    "\n",
    "```python\n",
    "from textblob import TextBlob\n",
    "\n",
    "text = TextBlob(\"I love this beautiful weather!\")\n",
    "print(text.sentiment.polarity)\n",
    "```\n",
    "\n",
    "ðŸ” Output:\n",
    "\n",
    "```\n",
    "0.85  # Very positive!\n",
    "```\n",
    "\n",
    "Another example:\n",
    "\n",
    "```python\n",
    "TextBlob(\"This is the worst movie ever.\").sentiment.polarity\n",
    "```\n",
    "\n",
    "ðŸ” Output:\n",
    "\n",
    "```\n",
    "-1.0  # Extremely negative\n",
    "```\n",
    "\n",
    "So, polarity helps you measure the *emotion* behind textâ€”perfect for reviews, feedback analysis, or even chatbot reactions!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b626d3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polarity: 0.625\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "text = \"I love this product!\"\n",
    "blob = TextBlob(text)\n",
    "print(\"Polarity:\", blob.sentiment.polarity)  # > 0 â†’ Positive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfba789f",
   "metadata": {},
   "source": [
    "2. Negative Sentiment\n",
    "Example Sentence: \"This is the worst service ever.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f26b67fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polarity: -1.0\n"
     ]
    }
   ],
   "source": [
    "text = \"This is the worst service ever.\"\n",
    "blob = TextBlob(text)\n",
    "print(\"Polarity:\", blob.sentiment.polarity)  # < 0 â†’ Negative\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1050f60",
   "metadata": {},
   "source": [
    " 3. Neutral Sentiment\n",
    "Example Sentence: \"It was okay, nothing special.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "913934b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polarity: 0.4285714285714286\n"
     ]
    }
   ],
   "source": [
    "text = \"It was okay, nothing special.\"\n",
    "blob = TextBlob(text)\n",
    "print(\"Polarity:\", blob.sentiment.polarity)  # â‰ˆ 0 â†’ Neutral"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef010aa4",
   "metadata": {},
   "source": [
    " 4. Mixed Sentiment\n",
    "Example Sentence: \"The food was great, but pricey.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aaaf9f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polarity: 0.8\n"
     ]
    }
   ],
   "source": [
    "text = \"The food was great, but pricey.\"\n",
    "blob = TextBlob(text)\n",
    "print(\"Polarity:\", blob.sentiment.polarity)  # Moderate â†’ Mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7caf07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixed Sentiment\n"
     ]
    }
   ],
   "source": [
    "if 0 < polarity < 0.4:\n",
    "    print(\"Mixed Sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672291c6",
   "metadata": {},
   "source": [
    " 5. Sarcasm/Irony (requires VADER or advanced models)\n",
    "Example Sentence: \"Oh great, another rainy day.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db0efe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6248c460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.155, 'neu': 0.357, 'pos': 0.488, 'compound': 0.5859}\n"
     ]
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "text = \"Oh great, another rainy day.\"\n",
    "score = analyzer.polarity_scores(text)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faeec91",
   "metadata": {},
   "source": [
    "6. Emotion-Based Tags (using NRCLex or similar)\n",
    "Example Sentence: \"I'm furious about this delay.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6083603b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nrclex in c:\\users\\pandit ji\\appdata\\roaming\\python\\python311\\site-packages (3.0.0)\n",
      "Requirement already satisfied: textblob in c:\\users\\pandit ji\\appdata\\roaming\\python\\python311\\site-packages (from nrclex) (0.19.0)\n",
      "Requirement already satisfied: nltk>=3.9 in c:\\users\\pandit ji\\appdata\\roaming\\python\\python311\\site-packages (from textblob->nrclex) (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\pandit ji\\appdata\\roaming\\python\\python311\\site-packages (from nltk>=3.9->textblob->nrclex) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\pandit ji\\appdata\\roaming\\python\\python311\\site-packages (from nltk>=3.9->textblob->nrclex) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\pandit ji\\appdata\\roaming\\python\\python311\\site-packages (from nltk>=3.9->textblob->nrclex) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pandit ji\\appdata\\roaming\\python\\python311\\site-packages (from nltk>=3.9->textblob->nrclex) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\pandit ji\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk>=3.9->textblob->nrclex) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\program files\\python311\\lib\\site-packages\\vboxapi-1.0-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "\n",
      "[notice] A new release of pip is available: 22.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install nrclex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79a16f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\PANDIT\n",
      "[nltk_data]     JI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49aaa6b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\PANDIT\n",
      "[nltk_data]     JI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('anger', 0.25), ('negative', 0.25), ('disgust', 0.25)]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "from nrclex import NRCLex\n",
    "\n",
    "text = \"I'm furious about this delay.\"\n",
    "emotion = NRCLex(text)\n",
    "print(emotion.top_emotions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc9d8f9",
   "metadata": {},
   "source": [
    " 7. Aspect-Based Sentiment\n",
    "Example Sentence: \"Camera is great, but battery life is poor.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d26ba54",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f1e288",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "text = TextBl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b567172b",
   "metadata": {},
   "source": [
    "# Entity Annotation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e4282e",
   "metadata": {},
   "source": [
    "2. Entity Annotation Programs (NER) in Python\n",
    "ðŸ”¹ 1. Basic Named Entity Recognition using SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f78b520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Inc. â†’ ORG\n",
      "Steve Jobs â†’ PERSON\n",
      "California â†’ GPE\n"
     ]
    }
   ],
   "source": [
    "# %pip install spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"Apple Inc. was founded by Steve Jobs in California.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \"â†’\", ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c098205e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fc3ce2f",
   "metadata": {},
   "source": [
    "# 2. highlight Entities in Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "209630ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'display' from 'IPython.core.display' (C:\\Users\\PANDIT JI\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\display.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mspacy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m displacy\n\u001b[32m      3\u001b[39m doc = nlp(\u001b[33m\"\u001b[39m\u001b[33mBarack Obama was born in Hawaii in 1961.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mdisplacy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43ment\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjupyter\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\spacy\\displacy\\__init__.py:69\u001b[39m, in \u001b[36mrender\u001b[39m\u001b[34m(docs, style, page, minify, jupyter, options, manual)\u001b[39m\n\u001b[32m     65\u001b[39m     html = RENDER_WRAPPER(html)\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m jupyter \u001b[38;5;129;01mor\u001b[39;00m (jupyter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m is_in_jupyter()):\n\u001b[32m     67\u001b[39m     \u001b[38;5;66;03m# return HTML rendered by IPython display()\u001b[39;00m\n\u001b[32m     68\u001b[39m     \u001b[38;5;66;03m# See #4840 for details on span wrapper to disable mathjax\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTML, display\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m display(HTML(\u001b[33m'\u001b[39m\u001b[33m<span class=\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtex2jax_ignore\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m>\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m</span>\u001b[39m\u001b[33m'\u001b[39m.format(html)))\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m html\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'display' from 'IPython.core.display' (C:\\Users\\PANDIT JI\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\display.py)"
     ]
    }
   ],
   "source": [
    "\n",
    "from spacy import displacy\n",
    "\n",
    "doc = nlp(\"Barack Obama was born in Hawaii in 1961.\")\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b271e9ed",
   "metadata": {},
   "source": [
    "3. Custom Entity Recognition (Rule-based with SpaCy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c91217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PS5 PRODUCT\n",
      "Amazon ORG\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Span\n",
    "\n",
    "text = \"Ravneet bought a PS5 from Amazon.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# Manually mark \"PS5\" as PRODUCT\n",
    "span = Span(doc, 3, 4, label=\"PRODUCT\")\n",
    "doc.ents = list(doc.ents) + [span]\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd75d93",
   "metadata": {},
   "source": [
    "4. Extract Specific Entity Types Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a607fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elon Musk â†’ PERSON\n",
      "the United States â†’ GPE\n"
     ]
    }
   ],
   "source": [
    "text = \"Elon Musk launched SpaceX in the United States.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ in [\"PERSON\", \"ORG\", \"GPE\"]:\n",
    "        print(ent.text, \"â†’\", ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b627d5a9",
   "metadata": {},
   "source": [
    "5. Entity Recognition with NLTK (Basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e451ce96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\PANDIT JI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808bf146",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\PANDIT JI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557628bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to C:\\Users\\PANDIT\n",
      "[nltk_data]     JI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to C:\\Users\\PANDIT\n",
      "[nltk_data]     JI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\PANDIT\n",
      "[nltk_data]     JI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\PANDIT JI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mmaxent_ne_chunker_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('maxent_ne_chunker_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mchunkers/maxent_ne_chunker_tab/english_ace_multiclass/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\PANDIT JI/nltk_data'\n    - 'c:\\\\Program Files\\\\Python311\\\\nltk_data'\n    - 'c:\\\\Program Files\\\\Python311\\\\share\\\\nltk_data'\n    - 'c:\\\\Program Files\\\\Python311\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\PANDIT JI\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m tags = pos_tag(tokens)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Perform Named Entity Recognition\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m tree = \u001b[43mne_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Print the named entity tree\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(tree)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\nltk\\chunk\\__init__.py:192\u001b[39m, in \u001b[36mne_chunk\u001b[39m\u001b[34m(tagged_tokens, binary)\u001b[39m\n\u001b[32m    190\u001b[39m     chunker = ne_chunker(fmt=\u001b[33m\"\u001b[39m\u001b[33mbinary\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     chunker = \u001b[43mne_chunker\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m chunker.parse(tagged_tokens)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\nltk\\chunk\\__init__.py:174\u001b[39m, in \u001b[36mne_chunker\u001b[39m\u001b[34m(fmt)\u001b[39m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mne_chunker\u001b[39m(fmt=\u001b[33m\"\u001b[39m\u001b[33mmulticlass\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    171\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    172\u001b[39m \u001b[33;03m    Load NLTK's currently recommended named entity chunker.\u001b[39;00m\n\u001b[32m    173\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMaxent_NE_Chunker\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfmt\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\nltk\\chunk\\named_entity.py:329\u001b[39m, in \u001b[36mMaxent_NE_Chunker.__init__\u001b[39m\u001b[34m(self, fmt)\u001b[39m\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m    328\u001b[39m \u001b[38;5;28mself\u001b[39m._fmt = fmt\n\u001b[32m--> \u001b[39m\u001b[32m329\u001b[39m \u001b[38;5;28mself\u001b[39m._tab_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mchunkers/maxent_ne_chunker_tab/english_ace_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfmt\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[38;5;28mself\u001b[39m.load_params()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mmaxent_ne_chunker_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('maxent_ne_chunker_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mchunkers/maxent_ne_chunker_tab/english_ace_multiclass/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\PANDIT JI/nltk_data'\n    - 'c:\\\\Program Files\\\\Python311\\\\nltk_data'\n    - 'c:\\\\Program Files\\\\Python311\\\\share\\\\nltk_data'\n    - 'c:\\\\Program Files\\\\Python311\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\PANDIT JI\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Ensure required NLTK resources are installed\n",
    "import nltk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')  # required for ne_chunk\n",
    "nltk.download('punkt')  # for word_tokenize\n",
    "nltk.download('averaged_perceptron_tagger')  # for pos_tag\n",
    "\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "\n",
    "text = \"Bill Gates founded Microsoft in 1975.\"\n",
    "\n",
    "# Tokenize text into words\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Tag words with part-of-speech labels\n",
    "tags = pos_tag(tokens)\n",
    "\n",
    "# Perform Named Entity Recognition\n",
    "tree = ne_chunk(tags)\n",
    "\n",
    "# Print the named entity tree\n",
    "print(tree)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
