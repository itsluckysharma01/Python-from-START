{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8aab7280",
   "metadata": {},
   "source": [
    "# **introduction to TextBlob in Python**\n",
    "\n",
    "---\n",
    "\n",
    "ðŸ‘‹ **Hey there!** Ready to explore how Python can understand human language? Letâ€™s meet your new buddy: **TextBlob**!\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ¤– What is TextBlob?\n",
    "\n",
    "Think of **TextBlob** as your smart assistant that can read and analyze text *almost like a human*â€”but faster and in Python! It helps with:\n",
    "\n",
    "* Figuring out what someone *feels* in a sentence (positive or negative?) ðŸŽ­\n",
    "* Splitting text into words and sentences âœ‚ï¸\n",
    "* Correcting spelling mistakes âœï¸\n",
    "* Translating from one language to another ðŸŒ\n",
    "* And even fixing grammar!\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§  Why use TextBlob?\n",
    "\n",
    "Because itâ€™s **super beginner-friendly**! You donâ€™t need to write complex code. Just one or two lines, and boomâ€”youâ€™ve got insights from text.\n",
    "\n",
    "---\n",
    "\n",
    "### âœ¨ Letâ€™s try it out!\n",
    "\n",
    "```python\n",
    "from textblob import TextBlob\n",
    "\n",
    "text = \"I really love programming with Python!\"\n",
    "blob = TextBlob(text)\n",
    "\n",
    "print(\"Words:\", blob.words)\n",
    "print(\"Sentiment:\", blob.sentiment)\n",
    "```\n",
    "\n",
    "ðŸ“Œ Output:\n",
    "\n",
    "```\n",
    "Words: ['I', 'really', 'love', 'programming', 'with', 'Python']\n",
    "Sentiment: Sentiment(polarity=0.9, subjectivity=0.6)\n",
    "```\n",
    "\n",
    "ðŸŽ¯ That `polarity` tells us the sentence is **very positive**!\n",
    "\n",
    "---\n",
    "\n",
    "Ready to explore more like **spelling correction** or **translation**?\n",
    "\n",
    "Try these next:\n",
    "\n",
    "```python\n",
    "TextBlob(\"I havv goo grammer.\").correct()\n",
    "blob.translate(to='hi')  # Translates to Hindi\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "TextBlob turns text into insights, effortlessly. A perfect tool if you're starting out in **Natural Language Processing**!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b03aaba",
   "metadata": {},
   "source": [
    "# 1. Text Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fc38ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "text = \"I love programming in Python. It's such a versatile language!\"\n",
    "blob = TextBlob(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fbcff2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polarity: 0.25\n",
      "Subjectivity: 0.55\n"
     ]
    }
   ],
   "source": [
    "# Get Polarity and Subjectivity\n",
    "print(\"Polarity:\", blob.sentiment.polarity)\n",
    "print(\"Subjectivity:\", blob.sentiment.subjectivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cd8d04",
   "metadata": {},
   "source": [
    "# 2. Detect posttive or negative sentiment in a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "681c7b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n",
      "Positive\n"
     ]
    }
   ],
   "source": [
    "def detect_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    if blob.sentiment.polarity > 0:\n",
    "        return \"Positive\"\n",
    "    elif blob.sentiment.polarity < 0:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "    \n",
    "print(detect_sentiment(\"i hate the interface\"))\n",
    "print(detect_sentiment('I love design'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9b79b0",
   "metadata": {},
   "source": [
    "# 3. Sentiment of Multiple Sentences (List of Reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2595668b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'I Love the product!' - Sentiment: Positive\n",
      "'Wrost camera ever!' - Sentiment: Neutral\n",
      "'Battery life is ok' - Sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "reviews = [\"I Love the product!\",\"Wrost camera ever!\",\"Battery life is ok\"]\n",
    "for review in reviews:\n",
    "    sentiment = TextBlob(review).sentiment.polarity\n",
    "    print(f\"'{review}' - Sentiment: {'Positive' if sentiment > 0 else 'Negative' if sentiment < 0 else 'Neutral'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7148c49",
   "metadata": {},
   "source": [
    "# 4. Count Positive/Negative/Netural in Text list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa38bc74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts: {'Positive': 3, 'Negative': 1, 'Neutral': 0}\n"
     ]
    }
   ],
   "source": [
    "reviews=[\"Great product!\",\"Worst experience ever!\",\"It's okay, not bad but not great.\", \"Not Bad\"]\n",
    "\n",
    "counts = {\"Positive\": 0, \"Negative\": 0, \"Neutral\": 0}\n",
    "for review in reviews:\n",
    "    polarity = TextBlob(review).sentiment.polarity\n",
    "    if polarity > 0:\n",
    "        counts[\"Positive\"] += 1\n",
    "    elif polarity < 0:\n",
    "        counts[\"Negative\"] += 1\n",
    "    else:\n",
    "        counts[\"Neutral\"] += 1\n",
    "print(\"Counts:\", counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad649205",
   "metadata": {},
   "source": [
    "# 5. Simple GUI For Sentiment (Using Tkinter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae39947c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tkinter import *\n",
    "# from textblob import TextBlob\n",
    "\n",
    "# def analyze():\n",
    "#     text = entry.get()\n",
    "#     blob = TextBlob(text)\n",
    "#     result.set(f\"Polarity: {blob.sentiment.polarity:.2f}\")\n",
    "\n",
    "# root = Tk()\n",
    "# root.title(\"Sentiment Analyzer\")\n",
    "\n",
    "# entry = Entry(root, width=40)\n",
    "# entry.pack(pady=10)\n",
    "\n",
    "\n",
    "# Button(root, text=\"Analyze\", command=analyze).pack()\n",
    "\n",
    "# result = StringVar()\n",
    "# Label(root, textvariable=result).pack(pady=5)\n",
    "\n",
    "# root.mainloop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0613c12",
   "metadata": {},
   "source": [
    "# Sentiment Annotation Types â€“ Python Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93565dd",
   "metadata": {},
   "source": [
    "In **TextBlob**, **polarity** is a number that tells you how **positive or negative** a piece of text is.\n",
    "\n",
    "### ðŸ“Š Polarity Score Range:\n",
    "\n",
    "* **-1.0** â†’ *very negative*\n",
    "* **0.0** â†’ *neutral*\n",
    "* **+1.0** â†’ *very positive*\n",
    "\n",
    "### ðŸ§  How it works:\n",
    "\n",
    "TextBlob uses a built-in sentiment lexicon (a list of words with associated emotions) to calculate the average sentiment of words in a sentence.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§ª Example:\n",
    "\n",
    "```python\n",
    "from textblob import TextBlob\n",
    "\n",
    "text = TextBlob(\"I love this beautiful weather!\")\n",
    "print(text.sentiment.polarity)\n",
    "```\n",
    "\n",
    "ðŸ” Output:\n",
    "\n",
    "```\n",
    "0.85  # Very positive!\n",
    "```\n",
    "\n",
    "Another example:\n",
    "\n",
    "```python\n",
    "TextBlob(\"This is the worst movie ever.\").sentiment.polarity\n",
    "```\n",
    "\n",
    "ðŸ” Output:\n",
    "\n",
    "```\n",
    "-1.0  # Extremely negative\n",
    "```\n",
    "\n",
    "So, polarity helps you measure the *emotion* behind textâ€”perfect for reviews, feedback analysis, or even chatbot reactions!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b626d3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polarity: 0.625\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "text = \"I love this product!\"\n",
    "blob = TextBlob(text)\n",
    "print(\"Polarity:\", blob.sentiment.polarity)  # > 0 â†’ Positive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfba789f",
   "metadata": {},
   "source": [
    "2. Negative Sentiment\n",
    "Example Sentence: \"This is the worst service ever.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f26b67fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polarity: -1.0\n"
     ]
    }
   ],
   "source": [
    "text = \"This is the worst service ever.\"\n",
    "blob = TextBlob(text)\n",
    "print(\"Polarity:\", blob.sentiment.polarity)  # < 0 â†’ Negative\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1050f60",
   "metadata": {},
   "source": [
    " 3. Neutral Sentiment\n",
    "Example Sentence: \"It was okay, nothing special.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "913934b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polarity: 0.4285714285714286\n"
     ]
    }
   ],
   "source": [
    "text = \"It was okay, nothing special.\"\n",
    "blob = TextBlob(text)\n",
    "print(\"Polarity:\", blob.sentiment.polarity)  # â‰ˆ 0 â†’ Neutral"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef010aa4",
   "metadata": {},
   "source": [
    " 4. Mixed Sentiment\n",
    "Example Sentence: \"The food was great, but pricey.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aaaf9f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polarity: 0.8\n"
     ]
    }
   ],
   "source": [
    "text = \"The food was great, but pricey.\"\n",
    "blob = TextBlob(text)\n",
    "print(\"Polarity:\", blob.sentiment.polarity)  # Moderate â†’ Mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7caf07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixed Sentiment\n"
     ]
    }
   ],
   "source": [
    "if 0 < polarity < 0.4:\n",
    "    print(\"Mixed Sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672291c6",
   "metadata": {},
   "source": [
    " 5. Sarcasm/Irony (requires VADER or advanced models)\n",
    "Example Sentence: \"Oh great, another rainy day.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db0efe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6248c460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.155, 'neu': 0.357, 'pos': 0.488, 'compound': 0.5859}\n"
     ]
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "text = \"Oh great, another rainy day.\"\n",
    "score = analyzer.polarity_scores(text)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faeec91",
   "metadata": {},
   "source": [
    "6. Emotion-Based Tags (using NRCLex or similar)\n",
    "Example Sentence: \"I'm furious about this delay.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6083603b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nrclex in c:\\users\\pandit ji\\appdata\\roaming\\python\\python311\\site-packages (3.0.0)\n",
      "Requirement already satisfied: textblob in c:\\users\\pandit ji\\appdata\\roaming\\python\\python311\\site-packages (from nrclex) (0.19.0)\n",
      "Requirement already satisfied: nltk>=3.9 in c:\\users\\pandit ji\\appdata\\roaming\\python\\python311\\site-packages (from textblob->nrclex) (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\pandit ji\\appdata\\roaming\\python\\python311\\site-packages (from nltk>=3.9->textblob->nrclex) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\pandit ji\\appdata\\roaming\\python\\python311\\site-packages (from nltk>=3.9->textblob->nrclex) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\pandit ji\\appdata\\roaming\\python\\python311\\site-packages (from nltk>=3.9->textblob->nrclex) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pandit ji\\appdata\\roaming\\python\\python311\\site-packages (from nltk>=3.9->textblob->nrclex) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\pandit ji\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk>=3.9->textblob->nrclex) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nrclex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79a16f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\PANDIT\n",
      "[nltk_data]     JI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49aaa6b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\PANDIT\n",
      "[nltk_data]     JI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('anger', 0.25), ('negative', 0.25), ('disgust', 0.25)]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "from nrclex import NRCLex\n",
    "\n",
    "text = \"I'm furious about this delay.\"\n",
    "emotion = NRCLex(text)\n",
    "print(emotion.top_emotions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc9d8f9",
   "metadata": {},
   "source": [
    " 7. Aspect-Based Sentiment\n",
    "Example Sentence: \"Camera is great, but battery life is poor.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4f1e288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Camera': 0.8, 'Battery': -0.4}\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "text = \"Camera is great, but battery life is poor.\"\n",
    "blob = TextBlob(text)\n",
    "\n",
    "# Aspect extraction (manually here)\n",
    "aspects = {\n",
    "    \"Camera\": TextBlob(\"Camera is great\").sentiment.polarity,\n",
    "    \"Battery\": TextBlob(\"battery life is poor\").sentiment.polarity\n",
    "}\n",
    "\n",
    "print(aspects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ea2d91",
   "metadata": {},
   "source": [
    "# Intent Annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5007484e",
   "metadata": {},
   "source": [
    "1. Basic Rule-Based intent detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "212e1e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greeting\n"
     ]
    }
   ],
   "source": [
    "def detect_intent(text):\n",
    "    text = text.lower()\n",
    "    if \"hi\" in text or \"hello\" in text:\n",
    "        return \"Greeting\"\n",
    "    elif \"bye\" in text or \"see you\" in text:\n",
    "        return \"Goodbye\"\n",
    "    elif \"order\" in text:\n",
    "        return \"OrderProduct\"\n",
    "    elif \"weather\" in text:\n",
    "        return \"GetWeather\"\n",
    "    elif \"remind\" in text:\n",
    "        return \"SetReminder\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "print(detect_intent(\"Hi, I want to check the weather.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa95da4",
   "metadata": {},
   "source": [
    "# 2. Using scikit-learn - Text classification (intent detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f055115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OrderProduct' 'GetWeather' 'GetWeather']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Training data\n",
    "texts = [\n",
    "    \"Hi there\", \"Goodbye\", \"I want to order pizza\",\n",
    "    \"What's the weather\", \"Remind me to study\"\n",
    "]\n",
    "labels = [\"Greeting\", \"Goodbye\", \"OrderProduct\", \"GetWeather\", \"SetReminder\"]\n",
    "\n",
    "# Vectorize and train\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(texts)\n",
    "model = MultinomialNB()\n",
    "model.fit(X, labels)\n",
    "\n",
    "# Prediction\n",
    "test = [\"Book pizza\", \"Hey!\", \"Will it rain tomorrow?\"]\n",
    "test_vector = vectorizer.transform(test)\n",
    "print(model.predict(test_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bec70d",
   "metadata": {},
   "source": [
    "# 3. Advanced BERT-based intent classification using transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8da3a04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': 'LABEL_0', 'score': 0.558242917060852}, {'label': 'LABEL_1', 'score': 0.44175708293914795}]]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model=\"bert-base-uncased\", top_k=None)\n",
    "\n",
    "input_text = \"I need to cancel my order\"\n",
    "output = classifier(input_text)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c09b3e",
   "metadata": {},
   "source": [
    "# 4. Intent Annotation on Multiple Inputs (Batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9c0f0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: What's the weather? â†’ Intent: GetWeather\n",
      "Input: Remind me to drink water â†’ Intent: SetReminder\n",
      "Input: Hi! â†’ Intent: Greeting\n",
      "Input: Cancel my booking â†’ Intent: Unknown\n"
     ]
    }
   ],
   "source": [
    "inputs = [\"What's the weather?\", \"Remind me to drink water\", \"Hi!\", \"Cancel my booking\"]\n",
    "\n",
    "for i in inputs:\n",
    "    print(f\"Input: {i} â†’ Intent: {detect_intent(i)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7357caa5",
   "metadata": {},
   "source": [
    "# 5. Simple GUI For Intent Dector (Tkinter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9dcccf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tkinter import *\n",
    "\n",
    "# def classify_intent():\n",
    "#     text = entry.get()\n",
    "#     intent = detect_intent(text)\n",
    "#     result.set(f\"Intent: {intent}\")\n",
    "\n",
    "# root = Tk()\n",
    "# root.title(\"Intent Detector\")\n",
    "\n",
    "# entry = Entry(root, width=40)\n",
    "# entry.pack(pady=10)\n",
    "\n",
    "# Button(root, text=\"Detect\", command=classify_intent).pack()\n",
    "\n",
    "# result = StringVar()\n",
    "# Label(root, textvariable=result).pack(pady=5)\n",
    "\n",
    "# root.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b567172b",
   "metadata": {},
   "source": [
    "# Entity Annotation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e4282e",
   "metadata": {},
   "source": [
    "2. Entity Annotation Programs (NER) in Python\n",
    "ðŸ”¹ 1. Basic Named Entity Recognition using SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f78b520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Inc. â†’ ORG\n",
      "Steve Jobs â†’ PERSON\n",
      "California â†’ GPE\n"
     ]
    }
   ],
   "source": [
    "# %pip install spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"Apple Inc. was founded by Steve Jobs in California.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \"â†’\", ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c098205e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fc3ce2f",
   "metadata": {},
   "source": [
    "# 2. Highlight Entities in Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "209630ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Barack Obama\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " was born in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Hawaii\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    1961\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ".</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from spacy import displacy\n",
    "\n",
    "# doc = nlp(\"Barack Obama was born in Hawaii in 1961.\")\n",
    "# displacy.render(doc, style=\"ent\")\n",
    "\n",
    "from spacy import displacy\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "doc = nlp(\"Barack Obama was born in Hawaii in 1961.\")\n",
    "html = displacy.render(doc, style=\"ent\", jupyter=False)  # Returns HTML\n",
    "display(HTML(html))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b271e9ed",
   "metadata": {},
   "source": [
    "3. Custom Entity Recognition (Rule-based with SpaCy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8c91217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PS5 PRODUCT\n",
      "Amazon ORG\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Span\n",
    "\n",
    "text = \"Ravneet bought a PS5 from Amazon.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# Manually mark \"PS5\" as PRODUCT\n",
    "span = Span(doc, 3, 4, label=\"PRODUCT\")\n",
    "doc.ents = list(doc.ents) + [span]\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd75d93",
   "metadata": {},
   "source": [
    "4. Extract Specific Entity Types Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81a607fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elon Musk â†’ PERSON\n",
      "the United States â†’ GPE\n"
     ]
    }
   ],
   "source": [
    "text = \"Elon Musk launched SpaceX in the United States.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ in [\"PERSON\", \"ORG\", \"GPE\"]:\n",
    "        print(ent.text, \"â†’\", ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b627d5a9",
   "metadata": {},
   "source": [
    "5. Entity Recognition with NLTK (Basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e451ce96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\PANDIT JI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "808bf146",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\PANDIT JI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "557628bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to C:\\Users\\PANDIT\n",
      "[nltk_data]     JI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     C:\\Users\\PANDIT JI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n",
      "[nltk_data] Downloading package words to C:\\Users\\PANDIT\n",
      "[nltk_data]     JI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\PANDIT\n",
      "[nltk_data]     JI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\PANDIT JI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Bill/NNP)\n",
      "  (PERSON Gates/NNP)\n",
      "  founded/VBD\n",
      "  (ORGANIZATION Microsoft/NNP)\n",
      "  in/IN\n",
      "  1975/CD\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# Ensure required NLTK resources are installed\n",
    "import nltk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('maxent_ne_chunker_tab')\n",
    "nltk.download('words')  # required for ne_chunk\n",
    "nltk.download('punkt')  # for word_tokenize\n",
    "nltk.download('averaged_perceptron_tagger')  # for pos_tag\n",
    "\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "\n",
    "text = \"Bill Gates founded Microsoft in 1975.\"\n",
    "\n",
    "# Tokenize text into words\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Tag words with part-of-speech labels\n",
    "tags = pos_tag(tokens)\n",
    "\n",
    "# Perform Named Entity Recognition\n",
    "tree = ne_chunk(tags)\n",
    "\n",
    "# Print the named entity tree\n",
    "print(tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d085ea",
   "metadata": {},
   "source": [
    "# Advnaced: Fine tuned Transformers for NER with Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c37b5b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n",
      "C:\\Users\\PANDIT JI\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\pipelines\\token_classification.py:181: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sundar Pichai â†’ PER\n",
      "Google â†’ ORG\n",
      "California â†’ LOC\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Use a smaller, memory-efficient NER model\n",
    "ner_pipeline = pipeline(\"ner\", model=\"dslim/bert-base-NER\", grouped_entities=True)\n",
    "text = \"Sundar Pichai is the CEO of Google based in California.\"\n",
    "\n",
    "results = ner_pipeline(text)\n",
    "for entity in results:\n",
    "    print(entity['word'], \"â†’\", entity['entity_group'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331b38c3",
   "metadata": {},
   "source": [
    "# Types of Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d34362",
   "metadata": {},
   "source": [
    "# 1. Basic Rule-Based Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fafbe9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General\n"
     ]
    }
   ],
   "source": [
    "def classify_text(text):\n",
    "    if \"buy now\" in text or \"win\" in text:\n",
    "        return \"Spam\"\n",
    "    elif \"hello\" in text or \"hi\" in text:\n",
    "        return \"Greeting\"\n",
    "    else:\n",
    "        return \"General\"\n",
    "\n",
    "print(classify_text(\"Win a free iPhone!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1a75c3",
   "metadata": {},
   "source": [
    "# 2. Using scikit-learn (TF-IDF +  Naive Bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "72c61909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Farewell' 'Farewell']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Training data\n",
    "texts = [\"I love this!\", \"This is terrible\", \"Win cash now!\", \"Hello there\", \"Goodbye\"]\n",
    "labels = [\"Positive\", \"Negative\", \"Spam\", \"Greeting\", \"Farewell\"]\n",
    "\n",
    "# Vectorize text\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Train model\n",
    "model = MultinomialNB()\n",
    "model.fit(X, labels)\n",
    "\n",
    "# Test prediction\n",
    "test = [\"You won a prize!\", \"That was amazing\"]\n",
    "X_test = vectorizer.transform(test)\n",
    "print(model.predict(X_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5437142c",
   "metadata": {},
   "source": [
    "# 3. Advanced: Using Hugging Face Transformers (BERT, RoBERTa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "156b609a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "249b2d0e0d5d4f699e0fe2b8f5581a71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PANDIT JI\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\PANDIT JI\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa2502daab4b4a69814ed3a8a7df569d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.999871015548706}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"text-classification\")\n",
    "result = classifier(\"I absolutely love the new update!\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93484c26",
   "metadata": {},
   "source": [
    "# 4. Multi-Class Text Classification (Custom Categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b26e34c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"The election results were announced\",\n",
    "    \"Ronaldo scored two goals\",\n",
    "    \"NASA launched a new satellite\"\n",
    "]\n",
    "labels = [\"Politics\", \"Sports\", \"Science\"]\n",
    "\n",
    "# Same vectorizer + classifier setup as above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff48aae",
   "metadata": {},
   "source": [
    "# 5. GUI-Based Text Classification (Tkinter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8ffb9191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tkinter import *\n",
    "\n",
    "# def classify():\n",
    "#     text = entry.get().lower()\n",
    "#     if \"win\" in text:\n",
    "#         result.set(\"Spam\")\n",
    "#     elif \"love\" in text:\n",
    "#         result.set(\"Positive\")\n",
    "#     else:\n",
    "#         result.set(\"Neutral\")\n",
    "\n",
    "# root = Tk()\n",
    "# root.title(\"Text Classifier\")\n",
    "\n",
    "# entry = Entry(root, width=50)\n",
    "# entry.pack(pady=10)\n",
    "\n",
    "# Button(root, text=\"Classify\", command=classify).pack()\n",
    "# result = StringVar()\n",
    "# Label(root, textvariable=result).pack(pady=5)\n",
    "\n",
    "# root.mainloop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5c9fb2",
   "metadata": {},
   "source": [
    "# Linguistic Annotation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433ecd79",
   "metadata": {},
   "source": [
    "with using SpaCy , NLTK and Stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86581f79",
   "metadata": {},
   "source": [
    "1. POS Tagging with SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "38ddf5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The â†’ DET\n",
      "quick â†’ ADJ\n",
      "brown â†’ ADJ\n",
      "fox â†’ NOUN\n",
      "jumps â†’ VERB\n",
      "over â†’ ADP\n",
      "the â†’ DET\n",
      "lazy â†’ ADJ\n",
      "dog â†’ NOUN\n",
      ". â†’ PUNCT\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, \"â†’\", token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300089da",
   "metadata": {},
   "source": [
    "2. Lemmatization using SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bb149058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The â†’ the\n",
      "quick â†’ quick\n",
      "brown â†’ brown\n",
      "fox â†’ fox\n",
      "jumps â†’ jump\n",
      "over â†’ over\n",
      "the â†’ the\n",
      "lazy â†’ lazy\n",
      "dog â†’ dog\n",
      ". â†’ .\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, \"â†’\", token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbaaf9b",
   "metadata": {},
   "source": [
    "3. Dependency Parsing with SpaCy (Syntax Tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "45666c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The â† fox ( det )\n",
      "quick â† fox ( amod )\n",
      "brown â† fox ( amod )\n",
      "fox â† jumps ( nsubj )\n",
      "jumps â† jumps ( ROOT )\n",
      "over â† jumps ( prep )\n",
      "the â† dog ( det )\n",
      "lazy â† dog ( amod )\n",
      "dog â† over ( pobj )\n",
      ". â† jumps ( punct )\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, \"â†\", token.head.text, \"(\", token.dep_, \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f5be08",
   "metadata": {},
   "source": [
    "4. POS Tagging with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "16880979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Ravneet', 'NNP'), ('is', 'VBZ'), ('writing', 'VBG'), ('code', 'NN'), ('.', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\PANDIT\n",
      "[nltk_data]     JI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\PANDIT JI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "sentence = \"Ravneet is writing code.\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tags = nltk.pos_tag(tokens)\n",
    "\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571897e0",
   "metadata": {},
   "source": [
    "5. Dependency and POS using Stanza(standford NLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9780a6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: stanza in c:\\users\\pandit ji\\appdata\\roaming\\python\\python311\\site-packages (1.10.1)\n",
      "Requirement already satisfied: emoji in c:\\users\\pandit ji\\appdata\\roaming\\python\\python311\\site-packages (from stanza) (2.14.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\pandit ji\\appdata\\roaming\\python\\python311\\site-packages (from stanza) (2.2.6)\n",
      "Requirement already satisfied: protobuf>=3.15.0 in c:\\users\\pandit ji\\appdata\\roaming\\python\\python311\\site-packages (from stanza) (4.25.8)\n",
      "Requirement already satisfied: requests in c:\\users\\pandit ji\\appdata\\roaming\\python\\python311\\site-packages (from stanza) (2.32.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\pandit ji\\appdata\\roaming\\python\\python311\\site-packages (from stanza) (3.5)\n",
      "Requirement already satisfied: torch>=1.3.0 in c:\\users\\pandit ji\\appdata\\roaming\\python\\python311\\site-packages (from stanza) (2.7.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pandit ji\\appdata\\roaming\\python\\python311\\site-packages (from stanza) (4.67.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\pandit ji\\appdata\\roaming\\python\\python311\\site-packages (from torch>=1.3.0->stanza) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\pandit ji\\appdata\\roaming\\python\\python311\\site-packages (from torch>=1.3.0->stanza) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\pandit ji\\appdata\\roaming\\python\\python311\\site-packages (from torch>=1.3.0->stanza) (1.14.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\pandit ji\\appdata\\roaming\\python\\python311\\site-packages (from torch>=1.3.0->stanza) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\pandit ji\\appdata\\roaming\\python\\python311\\site-packages (from torch>=1.3.0->stanza) (2025.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\pandit ji\\appdata\\roaming\\python\\python311\\site-packages (from sympy>=1.13.3->torch>=1.3.0->stanza) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\pandit ji\\appdata\\roaming\\python\\python311\\site-packages (from jinja2->torch>=1.3.0->stanza) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pandit ji\\appdata\\roaming\\python\\python311\\site-packages (from requests->stanza) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pandit ji\\appdata\\roaming\\python\\python311\\site-packages (from requests->stanza) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pandit ji\\appdata\\roaming\\python\\python311\\site-packages (from requests->stanza) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pandit ji\\appdata\\roaming\\python\\python311\\site-packages (from requests->stanza) (2024.7.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\pandit ji\\appdata\\roaming\\python\\python311\\site-packages (from tqdm->stanza) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# %pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737879ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8854be860e194319ac20bfa6f686ea39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-07 16:46:56 INFO: Downloaded file to C:\\Users\\PANDIT JI\\stanza_resources\\resources.json\n",
      "2025-07-07 16:46:56 INFO: Downloading default packages for language: en (English) ...\n",
      "2025-07-07 16:47:03 INFO: File exists: C:\\Users\\PANDIT JI\\stanza_resources\\en\\default.zip\n",
      "2025-07-07 16:47:07 INFO: Finished downloading models and saved to C:\\Users\\PANDIT JI\\stanza_resources\n",
      "2025-07-07 16:47:07 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a366d9180bf481fb00e0ef644fe85e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-07 16:47:08 INFO: Downloaded file to C:\\Users\\PANDIT JI\\stanza_resources\\resources.json\n",
      "2025-07-07 16:47:10 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2025-07-07 16:47:10 INFO: Using device: cpu\n",
      "2025-07-07 16:47:10 INFO: Loading: tokenize\n",
      "2025-07-07 16:47:10 INFO: Loading: mwt\n",
      "2025-07-07 16:47:10 INFO: Loading: pos\n",
      "2025-07-07 16:47:12 INFO: Loading: lemma\n",
      "2025-07-07 16:47:13 INFO: Loading: constituency\n",
      "2025-07-07 16:47:14 INFO: Loading: depparse\n",
      "2025-07-07 16:47:16 INFO: Loading: sentiment\n",
      "2025-07-07 16:47:18 INFO: Loading: ner\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline('en')\n",
    "\n",
    "doc = nlp(\"The girl read a book.\")\n",
    "for sent in doc.sentences:\n",
    "    for word in sent.words:\n",
    "        print(f\"{word.text} â†’ POS: {word.upos}, Dep: {word.deprel}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae7922d",
   "metadata": {},
   "source": [
    "6. Coreference Resolution (with HuggingFace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41cd0650dc4e4bc4b8f2586d0c1fc5dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:  11%|#         | 283M/2.64G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# âœ… Install Transformers (if needed in Google Colab)\n",
    "!pip install -q transformers\n",
    "\n",
    "# ðŸ§  Load Libraries\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "import re\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# ðŸ” Load Model and Tokenizer\n",
    "model_name = \"biu-nlp/lingmess-coref\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# âœï¸ Input Text\n",
    "text = \"Ravneet submitted her paper. She hopes the reviewers will appreciate her work.\"\n",
    "\n",
    "# ðŸ”¡ Tokenization\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# ðŸ“Š Prediction: Get token-level labels\n",
    "logits = outputs.logits\n",
    "predicted_labels = torch.argmax(logits, dim=-1).squeeze().tolist()\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"].squeeze())\n",
    "\n",
    "# ðŸ” Merge BPE tokens into readable words with labels\n",
    "def merge_tokens_and_labels(tokens, labels):\n",
    "    words, current_word, current_label = [], \"\", None\n",
    "    for token, label in zip(tokens, labels):\n",
    "        if token in [\"<s>\", \"</s>\"]:\n",
    "            continue\n",
    "        clean_token = token.replace(\"Ä \", \" \") if token.startswith(\"Ä \") else token\n",
    "        if token.startswith(\"Ä \") and current_word:\n",
    "            words.append((current_word.strip(), current_label))\n",
    "            current_word = clean_token\n",
    "            current_label = label\n",
    "        else:\n",
    "            current_word += clean_token\n",
    "            current_label = label\n",
    "    if current_word:\n",
    "        words.append((current_word.strip(), current_label))\n",
    "    return words\n",
    "\n",
    "resolved = merge_tokens_and_labels(tokens, predicted_labels)\n",
    "\n",
    "# ðŸ–¨ï¸ Print results\n",
    "print(\"ðŸ” Tokens + Coreference Labels\\n\" + \"-\" * 40)\n",
    "for word, label in resolved:\n",
    "    print(f\"{word:20} â†’ Label {label}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195c5ca9",
   "metadata": {},
   "source": [
    "Visulize coreference Labels in Color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9003dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŽ¨ HTML Highlight by Coreference Label\n",
    "def highlight_coref(text, resolved_words):\n",
    "    html = text\n",
    "    seen = set()\n",
    "    for word, label in resolved_words:\n",
    "        if label != 0 and word not in seen:\n",
    "            seen.add(word)\n",
    "            html = re.sub(rf\"\\b{re.escape(word)}\\b\",\n",
    "                          f\"<mark style='background-color:#ffff99'>{word}</mark>\", html)\n",
    "    return html\n",
    "\n",
    "highlighted = highlight_coref(text, resolved)\n",
    "display(HTML(f\"<p style='font-family:monospace;font-size:16px'>{highlighted}</p>\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
